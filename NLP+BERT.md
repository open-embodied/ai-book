# 返回 AI BOOK 主页

[返回 AI BOOK 主页](http://www.gitpp.com/ai1/ai-book)

大模型真的是太火了

# 大模型太火！！ 大语言模型改变世界，重燃AI热潮，其实NLP已经沉寂很多年


可以做很多很多大模型

1）法律大模型
[法律大模型](http://www.gitpp.com/labixiaoxin/legal-eagle-internlm)



##  RAG

1)RAG demo Spring AI+Ollama+pgvector实现本地RAG

[RAG demo Spring AI+Ollama+pgvector实现本地RAG](http://www.gitpp.com/spring808/rag-demo)


# 好书《零基础实战大模型》

# 从头开始学习大模型
[从头开始学习大模型](http://www.gitpp.com/ai1/ai-book/-/blob/main/nlp/llm.md)

# 基础开始

### 自然语言处理（NLP）

**1. 定义**

自然语言处理（NLP）是人工智能领域的重要研究方向，融合了语言学、计算机科学、机器学习、数学、认知心理学等多个学科领域的知识，是一门集计算机科学、人工智能和语言学于一体的交叉学科。它旨在使机器理解、解释并生成人类语言，实现人机之间有效沟通，使计算机能够执行语言翻译、情感分析、文本摘要等任务。

**2. 研究内容**

NLP的研究内容包括字、词、短语、句子、段落和篇章等多种层次，是机器语言和人类语言之间沟通的桥梁。它包含自然语言理解和自然语言生成两个主要方面。自然语言认知和理解是让计算机把输入的语言变成有意义的符号和关系，然后根据目的再处理。自然语言生成系统则是把计算机数据转化为自然语言。

**3. 主要任务**

* **文本预处理**：是NLP的一个重要步骤，包括文本清洗（去除HTML标签、特殊字符等）、分词（将文本划分为独立的词汇单元）、词性标注（确定每个词汇的词性）等。
* **词嵌入**：将词汇转换为计算机可理解的向量表示的过程。常见的词嵌入技术包括Word2Vec、GloVe等。这些技术可以捕捉词汇之间的语义关系，使计算机能够理解词汇的深层含义。
* **句法分析**：对用户输入的自然语言进行词汇短语的分析，目的是识别句子的句法结构，以实现自动句法分析的过程，包括短语结构分析（将句子划分为短语结构）和依存关系分析（确定词汇之间的依存关系）。
* **语义分析**：理解句子或文本深层含义的过程，包括实体识别（识别文本中的实体，如人名、地名等）、关系抽取（提取实体之间的关系）、情感分析（判断文本的情感倾向）等。语义分析涉及单词、词组、句子、段落所包含的意义，目的是用句子的语义结构来表示语言的结构。
* **文本生成**：接收结构化表示的语义，以输出符合语法的、流畅的、与输入语义一致的自然语言文本，这是NLP中的另一个重要任务，它可以根据给定的输入（如关键词、句子结构等）生成新的文本。这可以用于各种应用，如机器翻译、文本摘要、对话系统等。

**4. 发展历史**

* **早期基于规则的自然语言处理**：基于规则来建立词汇、句法语义分析、问答、聊天和机器翻译系统。它的优点是规则可以利用人类的内省知识，不依赖数据，可以快速起步；缺点是覆盖面不足，像个玩具系统，规则管理和可扩展一直没有解决。
* **统计自然语言处理（1990s-2000s）**：随着互联网的兴起，大量文本数据的出现推动了统计学习方法在自然语言处理中的应用。基于统计的机器学习（ML）开始流行，很多自然语言处理开始用机器学习算法，例如决策树，是硬性的、“如果-则”规则组成的系统，类似当时既有的人工定的规则。统计自然语言处理的主要思路是利用带标注的数据，基于人工定义的特征建立机器学习系统，并利用数据经过学习确定机器学习系统的参数。运行时利用这些学习得到的参数，对输入数据进行解码，得到输出。
* **神经网络自然语言处理（2010s至2024年）**：深度学习开始在语音和图像发挥威力。近来的研究更加聚焦于非监督式学习和半监督学习的算法。2016年，AlphaGo打败李世石；2017年Transformer模型诞生；2018年BERT模型推出，提出了预训练的方法。自2014年以来，人们尝试直接通过深度学习建模，进行端对端的训练。目前已在机器翻译、问答、阅读理解等领域取得了进展，出现了深度学习的热潮。

**5. 未来展望**

随着技术的不断进步和应用场景的不断扩展，NLP技术将呈现跨语言处理、个性化与智能化等发展趋势。




### Transformer


Transformer模型是一种深度学习架构，在自然语言处理（NLP）等领域带来了重大变革。以下是对它的介绍以及它对NLP的影响：

### Transformer模型介绍
- **提出背景**：Transformer模型由Ashish Vaswani等人在2017年的论文《Attention Is All You Need》中提出，旨在解决传统序列模型如循环神经网络（RNN）和卷积神经网络（CNN）在处理长序列数据时存在的一些问题，如长期依赖难以捕捉、并行计算能力有限等。
- **核心组件**
    - **多头注意力机制（Multi-Head Attention）**：这是Transformer的核心创新点。它允许模型同时关注输入序列中的不同位置信息，能够并行计算，高效地捕捉文本中的长距离依赖关系。其计算过程大致为：首先将输入的查询（Query）、键（Key）和值（Value）通过线性变换投影到多个头（head）中，然后在每个头中计算注意力得分，再通过加权求和得到每个头的输出，最后将多个头的输出拼接起来。
    - **位置编码（Positional Encoding）**：由于Transformer本身不具有对序列位置信息的天然感知能力，位置编码用于给输入序列添加位置信息，使模型能够区分不同位置的元素。一般采用正弦和余弦函数的组合来生成位置编码向量。
    - **编码器-解码器架构（Encoder-Decoder Architecture）**：Transformer由编码器和解码器两部分组成。编码器负责将输入序列编码成一个固定长度的向量表示，解码器则根据编码器的输出以及之前生成的输出序列来预测下一个输出。编码器和解码器都由多个堆叠的多头注意力层和前馈神经网络层组成，并使用了残差连接和层归一化等技术来提高训练的稳定性和效率。
- **优势**
    - **并行计算能力强**：相比RNN等需要顺序处理每个时间步的模型，Transformer可以并行计算所有位置的输出，大大提高了训练和推理速度。
    - **长序列处理能力出色**：能够有效地捕捉输入序列中的长距离依赖关系，对于处理长文本等任务表现出色。
    - **可解释性相对较好**：注意力机制可以直观地展示模型在处理文本时对不同位置的关注程度，具有一定的可解释性。

### 对自然语言处理带来的变革
- **语言理解能力大幅提升**
    - **在各类任务上取得突破**：在自然语言推理、文本分类、命名实体识别等任务中，基于Transformer的模型能够更准确地理解文本的语义和上下文信息，取得了显著优于传统模型的性能。例如，BERT模型在GLUE基准测试的多个任务上都达到了当时的最优成绩。
    - **推动预训练模型发展**：Transformer为预训练语言模型提供了强大的基础架构，使得大规模预训练模型如BERT、GPT系列等能够学习到丰富的语言知识，这些预训练模型在各种下游NLP任务中只需进行微调就能取得很好的效果，极大地改变了NLP的研究和应用范式。
- **语言生成质量显著提高**
    - **生成更自然流畅的文本**：在机器翻译、文本生成等任务中，Transformer能够生成更加自然、流畅和符合语法规则的文本。例如，在机器翻译中，基于Transformer的模型可以更好地处理不同语言之间的语法和语义差异，生成高质量的翻译结果。
    - **实现多样化的文本生成**：可以根据不同的上下文和任务要求，生成各种风格和类型的文本，如故事生成、对话生成等。GPT-3等模型能够生成令人惊叹的长篇文本，展现了Transformer在语言生成方面的强大能力。
- **跨模态融合成为可能**
    - **促进多模态任务发展**：Transformer的架构使得它能够方便地融合其他模态的信息，如视觉、语音等。这为多模态自然语言处理任务，如图像 captioning、视觉问答等提供了更有效的解决方案，推动了NLP与计算机视觉等领域的交叉研究。
- **模型压缩和优化技术发展**
    - **适应不同应用场景**：由于Transformer模型通常参数量巨大，为了使其能够在资源受限的设备上运行，研究人员开始致力于模型压缩和优化技术的研究，如量化、剪枝等。这些技术使得Transformer模型能够更好地应用于实际场景，如移动设备上的语音助手等。



### BERT

**1. 定义**

BERT，全称是 Bidirectional Encoder Representation from Transformers，是一种基于Transformer的预训练语言模型。BERT能够同时利用前后两个方向的信息，而ELMo和GPT只能使用单个方向的。

**2. 特点**

* **双向性**：BERT使用双向Transformer编码器结构，这意味着在预训练阶段，模型能够同时利用输入序列的左侧和右侧上下文信息，从而更准确地理解语言的含义。
* **深度双向预训练**：BERT被设计成一个深度双向模型，使得神经网络更有效地从第一层本身一直到最后一层捕获来自目标词的左右上下文的信息。
* **Transformer架构**：BERT使用Transformer的编码器部分，通过堆叠多个编码器层形成深度网络。每个编码器层包含自注意力机制、前馈神经网络等子层。
* **预训练与微调**：BERT首先在大量无标签文本数据上进行预训练，学习语言的通用表示。然后，针对特定NLP任务，使用标注数据对BERT模型进行微调，以适应特定任务的需求。

**3. 训练策略**

* **Masked Language Model（MLM）**：在预训练阶段，BERT随机掩盖输入序列中的部分词汇，并训练模型预测这些被掩盖词汇的原始值。这种方法迫使模型根据上下文信息来理解被掩盖词汇的含义。
* **Next Sentence Prediction（NSP）**：除了MLM外，BERT还通过预测两个句子之间的关系（是否是连续的句子）来进一步提升模型对句子间关系的理解能力。不过，在BERT的后续版本中，NSP任务被废弃了，因为研究人员发现它对下游任务的性能提升有限。

**4. 应用**

BERT在自然语言处理领域具有广泛的应用，包括但不限于：

* **文本分类**：如情感分析、垃圾邮件检测、主题分类等。
* **命名实体识别**：从文本中提取出具有特定意义的实体，如人名、地名、组织名等。
* **关系提取**：识别文本中实体之间的关系，如从新闻文章中提取出公司和CEO之间的关系。
* **问答系统**：包括阅读理解和问题回答任务，能够根据问题和文本段落提供相关的答案。
* **语义相似度计算**：计算文本之间的语义相似度，帮助理解文本之间的关联和差异。

**5. 优势**

* **强大的语言理解能力**：由于BERT采用了双向Transformer编码器结构，并在大量无标签文本数据上进行预训练，因此具有强大的语言理解能力。
* **易于迁移学习**：BERT模型可以针对多种NLP任务进行微调，而无需对模型架构进行重大改动。这使得BERT模型在各种NLP任务中表现出色，并易于迁移到其他领域和任务中。

**6. 局限性**

尽管BERT在自然语言处理领域取得了显著进展，但仍存在一些局限性。例如，BERT模型对计算资源的要求较高，训练和微调过程需要消耗大量时间和计算资源。此外，BERT模型在处理长文本时可能面临挑战，因为Transformer架构的自注意力机制在处理长序列时计算复杂度较高。

综上所述，自然语言处理（NLP）和BERT模型都是人工智能领域的重要研究方向和技术手段。NLP旨在使机器理解、解释并生成人类语言，而BERT模型则通过深度双向预训练的方式显著提升了自然语言处理的能力。随着技术的不断进步和应用场景的不断扩展，NLP和BERT模型将在更多领域发挥重要作用。



#大名鼎鼎  GPT



OpenAI开源GPT的过程并非简单直接，而是有着复杂的发展历程，以下是其主要的过程和相关阶段：

### 早期发展与技术积累
- **成立与初期研究**：OpenAI成立于2015年，由马斯克、山姆·阿尔特曼等人共同创立，其目标是推动人工智能的发展和普及。在成立初期，团队专注于深度学习和自然语言处理技术的研究，为后续GPT模型的开发奠定了基础。
- **技术突破与模型迭代**：OpenAI在自然语言处理领域不断探索和创新，通过大规模的数据集和先进的深度学习算法，逐渐提高了语言模型的性能。他们先后发布了多个语言模型，如GPT-1和GPT-2，这些模型在自然语言生成和理解方面取得了显著的成果。

### GPT-1的发布与初步影响
- **首次亮相**：2018年6月，OpenAI发布了GPT-1（Generative Pre-trained Transformer 1），这是一个基于Transformer架构的预训练语言模型。GPT-1在多个自然语言处理任务上表现出色，引起了学术界和工业界的广泛关注。
- **开源与合作**：OpenAI在发布GPT-1时，采取了相对开放的策略，向研究社区提供了模型的详细信息和代码，鼓励其他研究者基于该模型进行改进和应用开发。这一举措促进了自然语言处理领域的发展，也为OpenAI积累了更多的用户和合作机会。

### GPT-2的发布与争议
- **性能提升**：2019年2月，OpenAI发布了GPT-2，这是GPT-1的升级版，拥有更大的模型规模和更强的语言生成能力。GPT-2在生成文本的质量和连贯性方面有了显著的提升，能够生成接近人类水平的文章和对话。
- **部分开源与安全考虑**：由于GPT-2的强大能力可能被用于恶意用途，如生成虚假新闻和误导性信息，OpenAI在发布GPT-2时采取了谨慎的策略。他们只发布了部分模型参数和代码，而完整的模型则暂时保留，以评估其潜在的风险和影响。

### GPT-3的发布与商业应用
- **重大突破**：2020年5月，OpenAI发布了GPT-3，这是迄今为止最强大的语言模型之一。GPT-3拥有1750亿个参数，能够处理各种自然语言处理任务，如文本生成、问答系统、机器翻译等。GPT-3的发布引起了全球范围内的轰动，被认为是人工智能领域的重大突破。
- **商业策略转变**：与之前的模型不同，OpenAI在发布GPT-3时采取了更加商业化的策略。他们没有完全开源GPT-3的模型和代码，而是通过API的方式向开发者提供服务，开发者可以使用GPT-3的能力来开发各种应用程序。这一策略使得OpenAI能够更好地控制模型的使用和商业化，同时也为公司带来了可观的收入。

### 后续发展与展望
- **GPT系列的持续改进**：OpenAI继续对GPT系列模型进行改进和优化，不断提高模型的性能和效率。他们还推出了GPT-3.5和GPT-4等版本，进一步提升了语言模型的能力和应用范围。
- **开源与合作的平衡**：OpenAI在后续的发展中，继续探索开源与商业合作的平衡。他们一方面通过开源部分技术和工具，促进自然语言处理领域的发展；另一方面，通过商业化的方式，确保公司的可持续发展和技术创新。

总的来说，OpenAI开源GPT的过程是一个不断探索和调整的过程，他们在技术创新、安全考虑和商业利益之间寻求平衡，以推动人工智能的发展和应用。




# 入门

1)  300行代码实现minGPT

[300行代码实现minGPT](http://www.gitpp.com/zhangfei-ai/300-mingpt)


2）敢吗？啃一下这个代码 GPT2

[openAI GPT2 的源代码](http://www.gitpp.com/robot101/openai-gpt-2)


# 啃源代码

敢不敢啃源代码？想不想年薪百万？啃！

[GPT的源代码！啃！](http://lab.gitpp.com/html-docs/docs/zh/index.html)


# 实战


# 所有大模型的开源

[所有大模型的开源](http://www.gitpp.com/explore/projects/topics/LLM)


# 返回 AI BOOK 主页

[返回 AI BOOK 主页](http://www.gitpp.com/ai1/ai-book)


# GPT 开源的解读


GPT - 1（Generative Pretrained Transformer 1）是OpenAI开发的一款基于Transformer架构的预训练语言模型，以下为你详细解读其结构：

### 总体架构概述
GPT - 1采用了Transformer架构中的解码器部分。Transformer是一种基于注意力机制的深度学习模型，由编码器和解码器组成，而GPT - 1专注于解码器，这种架构使得它能够生成连贯的文本序列。

### 具体结构组件

#### 1. 输入嵌入层（Input Embedding）
- **词嵌入（Word Embedding）**：
    - 为了让模型能够处理文本，首先需要将输入的文本中的每个词转换为向量表示。GPT - 1使用词嵌入技术，将每个词映射到一个固定维度的向量空间中。例如，每个词可能被表示为一个长度为768的向量。这样的向量表示可以捕捉到词的语义信息，使得语义相近的词在向量空间中距离较近。
 - **位置嵌入（Position Embedding）**：
    - 由于Transformer架构本身不包含对序列中词的位置信息的感知，因此需要额外的位置嵌入来表示每个词在序列中的位置。位置嵌入同样是将位置索引映射到一个固定维度的向量，然后与词嵌入向量相加，得到最终的输入嵌入向量。这样模型就能够知道每个词在序列中的相对位置。

#### 2. 多层解码器块（Decoder Blocks）
GPT - 1包含12个相同的解码器块，每个解码器块都由两个主要子层组成：

- **多头自注意力机制（Multi - Head Self - Attention）**：
    - **原理**：自注意力机制允许模型在处理每个词时，考虑序列中其他词的信息。多头自注意力机制则是将自注意力机制并行地执行多次，每个头关注不同的信息表示。例如，一个头可能关注词之间的语义关系，另一个头可能关注词的语法结构。
    - **作用**：通过多头自注意力机制，模型能够捕捉到文本中不同层次和不同类型的依赖关系，从而更好地理解文本的上下文信息。
 - **前馈神经网络（Feed - Forward Neural Network）**：
    - **结构**：前馈神经网络由两个线性变换和一个非线性激活函数（通常是ReLU）组成。第一个线性变换将输入向量映射到一个更高维度的空间，然后通过ReLU激活函数引入非线性，最后再通过第二个线性变换将向量映射回原来的维度。
    - **作用**：前馈神经网络进一步对自注意力机制的输出进行处理和变换，提取更高级的特征表示。

每个解码器块还包含层归一化（Layer Normalization）操作，用于加速模型的训练和提高模型的稳定性。层归一化在每个子层的输入和输出之间进行，对每个样本的特征维度进行归一化处理。

#### 3. 输出层（Output Layer）
- **线性变换**：经过12个解码器块的处理后，最后一个解码器块的输出会通过一个线性变换，将向量的维度映射到词汇表的大小。
 - **Softmax激活函数**：线性变换的输出会经过Softmax激活函数，将其转换为一个概率分布，表示词汇表中每个词作为下一个生成词的概率。模型会根据这个概率分布选择概率最大的词作为生成的下一个词。


### 结构特点总结
- **单向语言模型**：GPT - 1是一个单向语言模型，在处理序列时，只关注当前词之前的上下文信息，这使得它在文本生成任务中能够自然地生成连贯的文本。
 - **预训练和微调**：GPT - 1采用了预训练和微调的两阶段训练方法。在预训练阶段，模型在大规模无监督文本数据上学习语言的通用模式；在微调阶段，模型在特定任务的有监督数据上进行微调，以适应不同的自然语言处理任务，如文本分类、问答系统等。 


# 啃源代码

敢不敢啃源代码？想不想年薪百万？啃！

[GPT的源代码！啃！](http://lab.gitpp.com/html-docs/docs/zh/index.html)



# 返回 AI BOOK 主页

[返回 AI BOOK 主页](http://www.gitpp.com/ai1/ai-book)
